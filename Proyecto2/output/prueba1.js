/** **** FILE AUTOGENERATED ***** */

const { RegParser } = require('../lib/FAClasses');
const { TOKEN_TYPE } = require('../lib/constants');
const LexicalAnalyzer = require('../lib/LexerClass/LexicalAnalyzer');

const filePath = process.argv.slice(2)[0];

const keyWordsRegEx = [{"name":"if","value":"if"}];
const tokensRegEx = [{"name":"id","value":"(a|b|c|d|e|f|g|h|i)((a|b|c|d|e|f|g|h|i)|(0|1))*"},{"name":"numero","value":"(0|1)((0|1))*"}];

// Create keyWords DFA
const keyWordsFSM = keyWordsRegEx.map((keyWord) => {
  const parser = new RegParser(keyWord.value);
  const { nfa } = parser.parseToNFA();
  const { dfa } = nfa.toDFA();

  return {
    name: keyWord.name,
    fsm: dfa,
  };
});

// Create tokens DFA
const tokensFSM = tokensRegEx.map((keyWord) => {
  const parser = new RegParser(keyWord.value);
  const { nfa } = parser.parseToNFA();
  const { dfa } = nfa.toDFA();

  return {
    name: keyWord.name,
    fsm: dfa,
  };
});

// Build Lexer
const lexer = new LexicalAnalyzer(filePath, keyWordsFSM, tokensFSM);

let token = lexer.nextToken();

while (token !== TOKEN_TYPE.END) {
  console.log(token);
  token = lexer.nextToken();
}
